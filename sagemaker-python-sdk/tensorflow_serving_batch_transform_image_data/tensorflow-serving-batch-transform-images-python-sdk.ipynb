{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Highly Performant TensorFlow Batch Inference and Training  \n",
    "\n",
    "For use cases involving large datasets, it may be necessary to perform highly performant batch inference using a cluster. In this notebook, we'll focus on how to use SageMaker batch transform to get inferences on a large datasets. To do this, we'll use a TensorFlow Serving model to do batch inference on a large dataset of images. We'll show how to use the new pre-processing and post-processing feature of the TensorFlow Serving container on Amazon SageMaker so that your TensorFlow model can make inferences directly on data in S3, and save post-processed inferences to S3.\n",
    "\n",
    "The dataset we'll be using is the [“Challenge 2018/2019\"] (https://github.com/cvdfoundation/open-images-dataset#download-the-open-images-challenge-28182019-test-set)” subset of the [Open Images V5 Dataset] (https://storage.googleapis.com/openimages/web/index.html). This subset consists of 100,00 images in .jpg format, for a total of 10GB. For demonstration, the [model] (https://github.com/tensorflow/models/tree/master/official/resnet#pre-trained-model) we'll be using is an image classification model based on the ResNet-50 architecture that has been trained on the ImageNet dataset, and which has been exported as a TensorFlow SavedModel.\n",
    "\n",
    "We will use this model to predict the class that each model belongs to. We'll write a pre- and post-processing script and package the script with our TensorFlow SavedModel, and demonstrate how to get inferences on large datasets with SageMaker batch transform quickly, efficiently, and at scale, on GPU-accelerated instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup \n",
    "\n",
    "We'll begin with some necessary imports, and get an Amazon SageMaker session to help perform certain tasks, as well as an IAM role with the necessary permissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region: us-west-2\n",
      "S3 URI: s3://sagemaker-us-west-2-038453126632/sagemaker/DEMO-tf-batch-inference-jpeg-images-python-sdk\n",
      "Role:   arn:aws:iam::038453126632:role/service-role/AmazonSageMaker-ExecutionRole-20180718T141171\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import os\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "\n",
    "region = sagemaker_session.boto_region_name\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = 'sagemaker/DEMO-tf-batch-inference-jpeg-images-python-sdk'\n",
    "print('Region: {}'.format(region))\n",
    "print('S3 URI: s3://{}/{}'.format(bucket, prefix))\n",
    "print('Role:   {}'.format(role))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting the SavedModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make inferences, we'll have to preprocess our image data in S3 to match the serving signature of our TensorFlow SavedModel (https://www.tensorflow.org/guide/saved_model), which we can inspect using the saved_model_cli (https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/tools/saved_model_cli.py).  This is the serving signature of the ResNet-50 v2 (NCHW, JPEG) (https://github.com/tensorflow/models/tree/master/official/resnet#pre-trained-model) model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download: s3://sagemaker-sample-data-us-west-2/batch-transform/open-images/model/resnet_v2_fp32_savedmodel_NCHW_jpg.tar.gz to ./resnet_v2_fp32_savedmodel_NCHW_jpg.tar.gz\n",
      "\n",
      "MetaGraphDef with tag-set: 'serve' contains the following SignatureDefs:\n",
      "\n",
      "signature_def['predict']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "    inputs['image_bytes'] tensor_info:\n",
      "        dtype: DT_STRING\n",
      "        shape: (-1)\n",
      "        name: input_tensor:0\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['classes'] tensor_info:\n",
      "        dtype: DT_INT64\n",
      "        shape: (-1)\n",
      "        name: ArgMax:0\n",
      "    outputs['probabilities'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 1001)\n",
      "        name: softmax_tensor:0\n",
      "  Method name is: tensorflow/serving/predict\n",
      "\n",
      "signature_def['serving_default']:\n",
      "  The given SavedModel SignatureDef contains the following input(s):\n",
      "    inputs['image_bytes'] tensor_info:\n",
      "        dtype: DT_STRING\n",
      "        shape: (-1)\n",
      "        name: input_tensor:0\n",
      "  The given SavedModel SignatureDef contains the following output(s):\n",
      "    outputs['classes'] tensor_info:\n",
      "        dtype: DT_INT64\n",
      "        shape: (-1)\n",
      "        name: ArgMax:0\n",
      "    outputs['probabilities'] tensor_info:\n",
      "        dtype: DT_FLOAT\n",
      "        shape: (-1, 1001)\n",
      "        name: softmax_tensor:0\n",
      "  Method name is: tensorflow/serving/predict\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp s3://sagemaker-sample-data-{region}/batch-transform/open-images/model/resnet_v2_fp32_savedmodel_NCHW_jpg.tar.gz .\n",
    "!tar -zxf resnet_v2_fp32_savedmodel_NCHW_jpg.tar.gz\n",
    "!saved_model_cli show --dir resnet_v2_fp32_savedmodel_NCHW_jpg/1538687370/ --all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SageMaker TensorFlow Serving Container uses the model’s SignatureDef named serving_default , which is declared when the TensorFlow SavedModel is exported. This SignatureDef says that the model accepts a string of arbitrary length as input, and responds with classes and their probabilities. With our image classification model, the input string will be a base-64 encoded string representing a JPEG image, which our SavedModel will decode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing a pre- and post-processing script\n",
    "\n",
    "We will package up our SavedModel with a Python script named `inference.py`, which will pre-process input data going from S3 to our TensorFlow Serving model, and post-process output data before it is saved back to S3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m# Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved.\u001b[39;49;00m\r\n",
      "\u001b[37m#\u001b[39;49;00m\r\n",
      "\u001b[37m# Licensed under the Apache License, Version 2.0 (the \"License\"). You\u001b[39;49;00m\r\n",
      "\u001b[37m# may not use this file except in compliance with the License. A copy of\u001b[39;49;00m\r\n",
      "\u001b[37m# the License is located at\u001b[39;49;00m\r\n",
      "\u001b[37m#\u001b[39;49;00m\r\n",
      "\u001b[37m#     http://aws.amazon.com/apache2.0/\u001b[39;49;00m\r\n",
      "\u001b[37m#\u001b[39;49;00m\r\n",
      "\u001b[37m# or in the \"license\" file accompanying this file. This file is\u001b[39;49;00m\r\n",
      "\u001b[37m# distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF\u001b[39;49;00m\r\n",
      "\u001b[37m# ANY KIND, either express or implied. See the License for the specific\u001b[39;49;00m\r\n",
      "\u001b[37m# language governing permissions and limitations under the License.\u001b[39;49;00m\r\n",
      "\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mbase64\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mio\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mjson\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mrequests\u001b[39;49;00m\r\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtf\u001b[39;49;00m\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mgoogle.protobuf.json_format\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m MessageToDict\r\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mstring\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m whitespace\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32minput_handler\u001b[39;49;00m(data, context):\r\n",
      "    \u001b[33m\"\"\" Pre-process request input before it is sent to TensorFlow Serving REST API\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m    Args:\u001b[39;49;00m\r\n",
      "\u001b[33m        data (obj): the request data stream\u001b[39;49;00m\r\n",
      "\u001b[33m        context (Context): an object containing request and configuration details\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m    Returns:\u001b[39;49;00m\r\n",
      "\u001b[33m        (dict): a JSON-serializable dict that contains request body and headers\u001b[39;49;00m\r\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\r\n",
      "\r\n",
      "    \u001b[34mif\u001b[39;49;00m context.request_content_type == \u001b[33m'\u001b[39;49;00m\u001b[33mapplication/x-tfexample\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m:\r\n",
      "        payload = data.read()\r\n",
      "        example = tf.train.Example()\r\n",
      "        example.ParseFromString(payload)\r\n",
      "        example_feature = MessageToDict(example.features)[\u001b[33m'\u001b[39;49;00m\u001b[33mfeature\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\r\n",
      "        encoded_image = example_feature[\u001b[33m'\u001b[39;49;00m\u001b[33mimage/encoded\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33mbytesList\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][\u001b[33m'\u001b[39;49;00m\u001b[33mvalue\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m][\u001b[34m0\u001b[39;49;00m]\r\n",
      "        instance = [{\u001b[33m\"\u001b[39;49;00m\u001b[33mb64\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: encoded_image}]\r\n",
      "        \u001b[34mreturn\u001b[39;49;00m json.dumps({\u001b[33m\"\u001b[39;49;00m\u001b[33minstances\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m: instance})\r\n",
      "    \u001b[34melse\u001b[39;49;00m:\r\n",
      "        _return_error(\u001b[34m415\u001b[39;49;00m, \u001b[33m'\u001b[39;49;00m\u001b[33mUnsupported content type \u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m{}\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(context.request_content_type \u001b[35mor\u001b[39;49;00m \u001b[33m'\u001b[39;49;00m\u001b[33mUnknown\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32moutput_handler\u001b[39;49;00m(response, context):\r\n",
      "    \u001b[33m\"\"\"Post-process TensorFlow Serving output before it is returned to the client.\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m    Args:\u001b[39;49;00m\r\n",
      "\u001b[33m        data (obj): the TensorFlow serving response\u001b[39;49;00m\r\n",
      "\u001b[33m        context (Context): an object containing request and configuration details\u001b[39;49;00m\r\n",
      "\u001b[33m\u001b[39;49;00m\r\n",
      "\u001b[33m    Returns:\u001b[39;49;00m\r\n",
      "\u001b[33m        (bytes, string): data to return to client, response content type\u001b[39;49;00m\r\n",
      "\u001b[33m    \"\"\"\u001b[39;49;00m\r\n",
      "    \u001b[34mif\u001b[39;49;00m response.status_code != \u001b[34m200\u001b[39;49;00m:\r\n",
      "        _return_error(response.status_code, response.content.decode(\u001b[33m'\u001b[39;49;00m\u001b[33mutf-8\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m))\r\n",
      "    response_content_type = context.accept_header\r\n",
      "    \u001b[37m# Remove whitespace from output JSON string.\u001b[39;49;00m\r\n",
      "    prediction = response.content.decode(\u001b[33m'\u001b[39;49;00m\u001b[33mutf-8\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m).translate(\u001b[36mdict\u001b[39;49;00m.fromkeys(\u001b[36mmap\u001b[39;49;00m(\u001b[36mord\u001b[39;49;00m,whitespace)))\r\n",
      "    \u001b[34mreturn\u001b[39;49;00m prediction, response_content_type\r\n",
      "\r\n",
      "\r\n",
      "\u001b[34mdef\u001b[39;49;00m \u001b[32m_return_error\u001b[39;49;00m(code, message):\r\n",
      "    \u001b[34mraise\u001b[39;49;00m \u001b[36mValueError\u001b[39;49;00m(\u001b[33m'\u001b[39;49;00m\u001b[33mError: {}, {}\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m.format(\u001b[36mstr\u001b[39;49;00m(code), message))\r\n"
     ]
    }
   ],
   "source": [
    "!pygmentize code/inference.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input_handler intercepts inference requests, base-64 encodes the request body, and formats the request body to conform to TensorFlow Serving’s REST API (https://www.tensorflow.org/tfx/serving/api_rest). The return value of the input_handler function is used as the request body in the TensorFlow Serving request. Binary data must use key \"b64\", according to the TFS REST API (https://www.tensorflow.org/tfx/serving/api_rest#encoding_binary_values), and since our serving signature’s input tensor has the suffix \"\\_bytes\", the encoded image data under key \"b64\" will be passed to the \"image\\_bytes\" tensor. Some serving signatures may accept a tensor of floats or integers instead of a base-64 encoded string, but for binary data (including image data), it is recommended that your SavedModel accept a base-64 encoded string for binary data, since JSON representations of binary data can be large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each incoming request originally contains a serialized JPEG image in its request body, and after passing through the input_handler, the request body contains the following, which our TensorFlow Serving accepts for inference:\n",
    "\n",
    "`{\"instances\": [{\"b64\":\"[base-64 encoded JPEG image]\"}]}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first field in the return value of `output_handler` is what SageMaker Batch Transform will save to S3 as this example’s prediction. In this case, our `output_handler` passes the content on to S3 unmodified.\n",
    "\n",
    "Pre- and post-processing functions let you perform inference with TensorFlow Serving on any data format, not just images. To learn more about the `input_handler` and `output_handler`, consult the SageMaker TensorFlow Serving Container README (https://github.com/aws/sagemaker-tensorflow-serving-container/blob/master/README.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packaging a Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After writing a pre- and post-processing script, you’ll need to package your TensorFlow SavedModel along with your script into a `model.tar.gz` file, which we’ll upload to S3 for the SageMaker TensorFlow Serving Container to use. Let's package the SavedModel with the `inference.py` script and examine the expected format of the `model.tar.gz` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "code/\n",
      "code/.ipynb_checkpoints/\n",
      "code/inference.py\n",
      "1538687370/\n",
      "1538687370/saved_model.pb\n",
      "1538687370/variables/\n",
      "1538687370/variables/variables.data-00000-of-00001\n",
      "1538687370/variables/variables.index\n"
     ]
    }
   ],
   "source": [
    "!tar -cvzf model.tar.gz code --directory=resnet_v2_fp32_savedmodel_NCHW_jpg 1538687370"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`1538687370` refers to the model version number of the SavedModel, and this directory contains our SavedModel artifacts. The code directory contains our pre- and post-processing script, which must be named `inference.py`. I can also include an optional `requirements.txt` file, which is used to install dependencies with `pip` from the Python Package Index before the Transform Job starts, but we don’t need any additional dependencies in this case, so we don't include a requirements file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use this `model.tar.gz` when we create a SageMaker Model, which we will use to run Transform Jobs. To learn more about packaging a model, you can consult the SageMaker TensorFlow Serving Container README (https://github.com/aws/sagemaker-tensorflow-serving-container/blob/master/README.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run a Batch Transform job\n",
    "\n",
    "Next, we'll run a Batch Transform job using our data processing script and GPU-based Amazon SageMaker Model. More specifically, we'll perform inference on a cluster of two instances, though we can choose more or fewer. The objects in the S3 path will be distributed between the instances. In this example, our input can't be split by newline characters and batched together, so the cluster will receive one HTTP request per serialized image.\n",
    "\n",
    "The code below creates a SageMaker Model entity that will be used for Batch inference, and runs a Transform Job using that Model. The Model contains a reference to the TFS container, and the `model.tar.gz` containing our TensorFlow SavedModel and the pre- and post-processing `inference.py` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "from sagemaker.tensorflow.serving import Model\n",
    "\n",
    "s3_path = 's3://{}/{}'.format(bucket, prefix)\n",
    "\n",
    "model_data = sagemaker_session.upload_data('model.tar.gz',\n",
    "                                           bucket,\n",
    "                                           os.path.join(prefix, 'model'))\n",
    "                                           \n",
    "tensorflow_serving_model = Model(model_data=model_data,\n",
    "                                 role=role,\n",
    "                                 framework_version='1.13',\n",
    "                                 sagemaker_session=sagemaker_session)\n",
    "\n",
    "input_path = 's3://sagemaker-sample-data-{}/batch-transform/open-images/jpg'.format(region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we create a Transform Job, let's inspect some of our input data. Here's an example, the first image in our dataset:\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"sample_image/00000b4dcff7f799.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data in the input path consists of 100,000 JPEG images of varying sizes and shapes. Here is a subset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform input path: s3://sagemaker-sample-data-us-west-2/batch-transform/open-images/jpg\n",
      "2019-07-09 22:19:18  126.2 KiB 00000b4dcff7f799.jpg\n",
      "2019-07-09 22:19:18  115.8 KiB 00001a21632de752.jpg\n",
      "2019-07-09 22:19:18  151.0 KiB 0000d67245642c5f.jpg\n",
      "2019-07-09 22:19:18  159.9 KiB 0001244aa8ed3099.jpg\n",
      "2019-07-09 22:19:18  115.0 KiB 000172d1dd1adce0.jpg\n",
      "2019-07-09 22:19:18   65.4 KiB 0001c8fbfb30d3a6.jpg\n",
      "2019-07-09 22:19:18   70.4 KiB 0001dd930912683d.jpg\n",
      "2019-07-09 22:19:18   73.0 KiB 0002c96937fae3b3.jpg\n",
      "2019-07-09 22:19:18  109.2 KiB 0002f94fe2d2eb9f.jpg\n",
      "2019-07-09 22:19:18  119.2 KiB 000305ba209270dc.jpg\n",
      "2019-07-09 22:19:18  119.5 KiB 000313fed9979d24.jpg\n",
      "2019-07-09 22:19:18   77.2 KiB 0003a523fa9b2a3f.jpg\n",
      "2019-07-09 22:19:18   84.9 KiB 0003d1c3be9ed3d6.jpg\n",
      "2019-07-09 22:19:18   82.9 KiB 000455be7b222c04.jpg\n",
      "2019-07-09 22:19:18  104.8 KiB 0004fdbc5b94c7c2.jpg\n",
      "2019-07-09 22:19:18  144.0 KiB 0005339c44e6071b.jpg\n",
      "2019-07-09 22:19:19   75.2 KiB 0005aea8c9144c77.jpg\n",
      "2019-07-09 22:19:19   71.0 KiB 0005facd8dbaf39a.jpg\n",
      "2019-07-09 22:19:19   47.5 KiB 0005fcc2fd4eaa4a.jpg\n",
      "2019-07-09 22:19:19  151.3 KiB 00069966ba8519e5.jpg\n",
      "2019-07-09 22:19:19   85.1 KiB 0007c05c94bcf41b.jpg\n",
      "2019-07-09 22:19:19  111.0 KiB 0007e8279612ac07.jpg\n",
      "2019-07-09 22:19:19   62.7 KiB 0007ffd0ba02c9ce.jpg\n",
      "2019-07-09 22:19:19   80.5 KiB 0008348dfbd5d85f.jpg\n",
      "2019-07-09 22:19:19  104.3 KiB 0008501b0c5108ab.jpg\n",
      "2019-07-09 22:19:19  205.2 KiB 0008525fb2879723.jpg\n",
      "2019-07-09 22:19:19  178.4 KiB 0008758042ec63a7.jpg\n",
      "2019-07-09 22:19:19   77.2 KiB 000912f449df9180.jpg\n",
      "2019-07-09 22:19:19   41.4 KiB 000a017e2b7063a7.jpg\n",
      "2019-07-09 22:19:19  107.7 KiB 000b39037205b35d.jpg\n",
      "2019-07-09 22:19:19  180.7 KiB 000cb33296bfd7f1.jpg\n",
      "2019-07-09 22:19:19   59.5 KiB 000d3e755ce7542c.jpg\n",
      "2019-07-09 22:19:19   85.5 KiB 000d6aba3b0af069.jpg\n",
      "2019-07-09 22:19:19  225.5 KiB 000f18da4c446aab.jpg\n",
      "2019-07-09 22:19:19   85.3 KiB 000f579533500448.jpg\n",
      "2019-07-09 22:19:19  135.8 KiB 000fbd342baaadd3.jpg\n",
      "2019-07-09 22:19:19   87.6 KiB 000fdfc1f01572d4.jpg\n"
     ]
    }
   ],
   "source": [
    "!echo \"Transform input path: {input_path}\"\n",
    "!aws s3 ls {input_path}/000 --human-readable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we’ve created a SageMaker Model, we can use it to run batch predictions using Batch Transform. We specify the input S3 data, content type of the input data, the output S3 data, and instance type and count.\n",
    "\n",
    "For improved performance, we specify two additional parameters `max_concurrent_transforms` and `max_payload`, which control the maximum number of parallel requests that can be sent to each instance in a transform job at a time, and the maximum size of each request body.\n",
    "\n",
    "When performing inference on entire S3 objects that cannot be split by newline characters, such as images, it is recommended that you set `max_payload` to be slightly larger than the largest S3 object in your dataset, and that you experiment with the `max_concurrent_transforms` parameter in powers of two to find a value that maximizes throughput for your model. For example, we’ve set `max_concurrent_transforms` to 64 after experimenting with powers of two, and we set `max_payload` to 1, since the largest object in our S3 input is less than one megabyte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform input S3 path:  s3://sagemaker-sample-data-us-west-2/batch-transform/open-images/jpg\n",
      "Transform output S3 path: s3://sagemaker-us-west-2-038453126632/sagemaker/DEMO-tf-batch-inference-jpeg-images-python-sdk/output\n",
      ".........................................................................................................................................................................................................................."
     ]
    }
   ],
   "source": [
    "output_path = os.path.join(s3_path, 'output')\n",
    "tensorflow_serving_transformer = tensorflow_serving_model.transformer(\n",
    "                                     instance_count=2,\n",
    "                                     instance_type='ml.p2.xlarge',\n",
    "                                     max_concurrent_transforms=64,\n",
    "                                     max_payload=1,\n",
    "                                     output_path=output_path)\n",
    "\n",
    "print('Transform input S3 path:  {}'.format(input_path))\n",
    "print('Transform output S3 path: {}'.format(output_path))\n",
    "tensorflow_serving_transformer.transform(input_path, content_type='application/x-image')\n",
    "tensorflow_serving_transformer.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After our transform job finishes, we find one S3 object in the output path for each object in the input path. This object contains the inferences from our model for that object, and has the same name as the corresponding input object, but with `.out` appended to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls {output_path}/000 --human-readable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting one of the output objects, we find the prediction from our TensorFlow Serving model. This is from the example image displayed above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp {output_path}/00000b4dcff7f799.jpg.out .\n",
    "!cat 00000b4dcff7f799.jpg.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".........................................................!\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.tensorflow.serving import Model\n",
    "\n",
    "tensorflow_serving_model = Model(source_dir='code',\n",
    "                                 entry_point='code/inference.py',\n",
    "                                 model_data=estimator_dist.model_data,\n",
    "                                 role=role,\n",
    "                                 framework_version='1.12',\n",
    "                                 sagemaker_session=sagemaker_session)\n",
    "\n",
    "input_data_path='s3://sagemaker-sample-data-{}/tensorflow/cifar10/images/png'.format(sagemaker_session.boto_region_name)\n",
    "output_data_path='s3://{}/{}/{}'.format(bucket, prefix, 'batch-predictions')\n",
    "batch_instance_count=2\n",
    "batch_instance_type = 'ml.p2.xlarge'\n",
    "concurrency=32\n",
    "max_payload_in_mb=1\n",
    "\n",
    "transformer = tensorflow_serving_model.transformer(\n",
    "    instance_count=batch_instance_count,\n",
    "    instance_type=batch_instance_type,\n",
    "    max_concurrent_transforms=concurrency,\n",
    "    max_payload=max_payload_in_mb,\n",
    "    output_path=output_data_path\n",
    ")\n",
    "\n",
    "transformer.transform(data=input_data_path, content_type='application/x-image')\n",
    "transformer.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Batch Transform output\n",
    "\n",
    "Finally, we can inspect the output files of our Batch Transform job to see the predictions.  First we'll download the prediction files locally, then extract the predictions from them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp --quiet --recursive $transformer.output_path ./batch_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "total = 0\n",
    "correct = 0\n",
    "predicted = []\n",
    "actual = []\n",
    "labels = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
    "for entry in os.scandir('batch_predictions'):\n",
    "    try:\n",
    "        if entry.is_file() and entry.name.endswith(\"out\"):\n",
    "            with open(entry, 'r') as f:\n",
    "                jstr = json.load(f)\n",
    "                results = [float('%.3f'%(item)) for sublist in jstr['predictions'] for item in sublist]\n",
    "                class_index = np.argmax(np.array(results))\n",
    "                predicted_label = labels[class_index]\n",
    "                predicted.append(predicted_label)\n",
    "                actual_label = re.search('([a-zA-Z]+).png.out', entry.name).group(1)\n",
    "                actual.append(actual_label)\n",
    "                is_correct = (predicted_label in entry.name) or False\n",
    "                if is_correct:\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate the accuracy of the predictions.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Out of {} total images, accurate predictions were returned for {}'.format(total, correct))\n",
    "accuracy = correct / total\n",
    "print('Accuracy is {:.1%}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy from the batch transform job on 10000 test images never seen during training is fairly close to the accuracy achieved during training on the validation set.  This is an indication that the model is not overfitting and should generalize fairly well to other unseen data. \n",
    "\n",
    "Next we'll plot a confusion matrix, which is a tool for visualizing the performance of a multiclass model. It has entries for all possible combinations of correct and incorrect predictions, and shows how often each one was made by our model. Ours will be row-normalized: each row sums to one, so that entries along the diagonal correspond to recall. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "confusion_matrix = pd.crosstab(pd.Series(actual), pd.Series(predicted), rownames=['Actuals'], colnames=['Predictions'], normalize='index')\n",
    "sns.heatmap(confusion_matrix, annot=True, fmt='.2f', cmap=\"YlGnBu\").set_title('Confusion Matrix')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our model had 100% accuracy, and therefore 100% recall in every class, then all of the predictions would fall along the diagonal of the confusion matrix.  Here our model definitely is not 100% accurate, but manages to achieve good recall for most of the classes, though it performs worse for some classes, such as cats.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deployment with Amazon Elastic Inference\n",
    "\n",
    "Amazon SageMaker also lets you deploy a TensorFlow Serving model to a hosted Endpoint for real-time inference. As we will see, the processes for setting up hosted endpoints and Batch Transform jobs have significant differences.  Additionally, we will discuss why and how to use Amazon Elastic Inference with the hosted endpoint.\n",
    "\n",
    "### Deploying the Model\n",
    "\n",
    "When considering the overall cost of a machine learning workload, inference often is the largest part, up to 90% of the total.  If a GPU instance type is used for real time inference, it typically is not fully utilized because, unlike training, real time inference does not involve continuously inputting large batches of data to the model.  Elastic Inference provides GPU acceleration suited for inference, allowing you to add inference acceleration to a hosted endpoint for a fraction of the cost of using a full GPU instance.\n",
    "\n",
    "Instead of a Transformer object, we'll instantiate a Predictor object now. The `deploy` method of the Estimator object instantiates a Predictor object representing an endpoint which serves prediction requests in near real time.  To utilize Elastic Inference with the SageMaker TFS container, simply provide an `accelerator_type` parameter, which determines the type of accelerator that is attached to your endpoint. Refer to the **Inference Acceleration** section of the [instance types chart](https://aws.amazon.com/sagemaker/pricing/instance-types) for a listing of the supported types of accelerators. \n",
    "\n",
    "Here we'll use a general purpose CPU compute instance type along with an Elastic Inference accelerator:  together they are much cheaper than the smallest P3 GPU instance type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = estimator_dist.deploy(initial_instance_count=1,\n",
    "                                  instance_type='ml.m5.xlarge',\n",
    "                                  accelerator_type='ml.eia1.medium')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Real time inference\n",
    "  \n",
    "Now that we have a Predictor object wrapping a real time Amazon SageMaker hosted endpoint, we'll define the label names and look at a sample of 10 images, one from each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "images = []\n",
    "for entry in os.scandir('sample-img'):\n",
    "    if entry.is_file() and entry.name.endswith(\"png\"):\n",
    "        images.append('sample-img/' + entry.name)\n",
    "\n",
    "for image in images:\n",
    "    display(Image(image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll modify some properties of the Predictor object created by the `deploy` method call above.  The TFS container in Amazon SageMaker by default uses the TFS REST API, which requires requests in a specific JSON format.  However, for many real time use cases involving image data it is more convenient to have the client application send the image data directly to an endpoint for predictions, without converting and preprocessing it on the client side. \n",
    "\n",
    "Fortunately, our endpoint includes the same pre/post-processing script used in the Batch Transform section of this notebook because the same model artifact is used in both cases. This model artifact includes the same `inference.py` script.  With this preprocessing script in place, we just specify the Predictor's content type as `application/x-image` and override the default serializer. Then we can simply provide the raw .png image bytes to the Predictor.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.content_type = 'application/x-image'\n",
    "predictor.serializer = None\n",
    "\n",
    "labels = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
    "\n",
    "def get_prediction(file_path):\n",
    "    \n",
    "    with open(file_path, \"rb\") as image:\n",
    "        f = image.read()\n",
    "    b = bytearray(f)\n",
    "    return labels[np.argmax(predictor.predict(b)['predictions'], axis=1)[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [get_prediction(image) for image in images]\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extensions\n",
    "\n",
    "Although we did not demonstrate them in this notebook, Amazon SageMaker provides additional ways to make distributed training more efficient for very large datasets:\n",
    "- **VPC training**:  performing Horovod training inside a VPC improves the network latency between nodes, leading to higher performance and stability of Horovod training jobs.\n",
    "\n",
    "- **Pipe Mode**:  using [Pipe Mode](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo.html#your-algorithms-training-algo-running-container-inputdataconfig) reduces startup and training times.  Pipe Mode streams training data from S3 as a Linux FIFO directly to the algorithm, without saving to disk.  For a small dataset such as CIFAR-10, Pipe Mode does not provide any advantage, but for very large datasets where training is I/O bound rather than CPU/GPU bound, Pipe Mode can substantially reduce startup and training times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleanup\n",
    "\n",
    "To avoid incurring charges due to a stray endpoint, delete the Amazon SageMaker endpoint if you no longer need it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session.delete_endpoint(predictor.endpoint)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_tensorflow_p36",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "notice": "Copyright 2017 Amazon.com, Inc. or its affiliates. All Rights Reserved.  Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
